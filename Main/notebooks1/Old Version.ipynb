{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3052ca",
   "metadata": {},
   "source": [
    "# Consumer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f122bf8",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dea42b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9491\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"CuSn1JFQTEt2cK_C...|I went to New Cit...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('json-changes-event-consumer')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,mysql:mysql-connector-java:8.0.11\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9094ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"185.185.126.143:9092\") # kafka server\n",
    "  .option(\"subscribe\", \"ML_Topic\") # topic name matching the producer \n",
    "  .option(\"startingOffsets\", \"latest\") # start from beginning select  \"latest\" or earliest\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2f890b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "221385c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comma seprated is dangerous because text column has many commas. \n",
    "df2=df1\\\n",
    "      .selectExpr(\"split(value,',')[0] as review_id\" \\\n",
    "                  ,\"split(value,',')[1] as text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1660e3",
   "metadata": {},
   "source": [
    "#clearing checkpoints \n",
    "spark.cleaner.referenceTracking.cleanCheckpoints  true\n",
    "\n",
    "or \n",
    "\n",
    "rm -f . . ipynb_checkpoints/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f04c60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/30 12:16:44 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ffc4085d-d23b-4148-b685-c0c4ff682740. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fe7d91d73a0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9492\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"rRQCXz5JXCAJXZqs...|My budgie was sic...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+---------+----+\n",
      "|review_id|text|\n",
      "+---------+----+\n",
      "+---------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9494:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"s82DoR3FSIWbcmrK...|I have so many wa...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9493\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"s82DoR3FSIWbcmrK...|I have so many wa...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check data flow \n",
    "df2.writeStream.outputMode(\"append\") \\\n",
    "            .format(\"console\") \\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89288704",
   "metadata": {},
   "source": [
    "# Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2cb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9496\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"N66pdRhzayxvq-5H...|We took our dog L...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"N66pdRhzayxvq-5H...|We took our dog L...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9497\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"YP7UT1pILhR_QeuW...|Have been twice a...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"YP7UT1pILhR_QeuW...|Have been twice a...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"WGi-THHfUruzjUT2...|I ever only get t...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9498\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"WGi-THHfUruzjUT2...|I ever only get t...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"YRWyWgDEHwk_Q7cp...|Ok place to eat. ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9499\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"YRWyWgDEHwk_Q7cp...|Ok place to eat. ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"ilPMwm0CNYcsxPpG...|I love this place...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9500\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"ilPMwm0CNYcsxPpG...|I love this place...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9510:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0DEp3FSfy3vRwunM...|I've only been he...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9501\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0DEp3FSfy3vRwunM...|I've only been he...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d40d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aca3c3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           80000     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                12544     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                1560      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 94,129\n",
      "Trainable params: 94,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "-------------------------------------------\n",
      "Batch: 3264\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"TlUwbT3IuHQ6cu9S...|Who would have th...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3265:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3265\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"GMr7pPENsEEwLTH1...|Via Lago is a gre...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model('NLP_Comments_Classification_20200531.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5493cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizer_X_20200531.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('tokenizer_Y_20200531.pickle', 'rb') as handle:\n",
    "    label_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5261940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9502\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"_r28zHxLg1m_m9mp...|J. GIlbert's has ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"_r28zHxLg1m_m9mp...|J. GIlbert's has ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9514:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9503\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"_SpjTgW_h48I5CBI...|We were staying a...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"_SpjTgW_h48I5CBI...|We were staying a...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Should output three fields: Probability , the outcome and the full message \n",
    "##list with three elements \n",
    "\n",
    "def modelevdh(textbody) : \n",
    "        phrase=list([textbody])\n",
    "        x = tokenizer.texts_to_sequences(phrase)\n",
    "        paddedx = pad_sequences(x, maxlen=120, padding='post', truncating='post')\n",
    "        probability=model.predict_proba(paddedx).max(axis=1) \n",
    "       # print(phrase)\n",
    "        if probability==0:\n",
    "            my_prediction= \"No Classification\"\n",
    "        elif probability<=.3:\n",
    "            my_prediction=\"Accepted\"\n",
    "        elif probability>=.7:\n",
    "            my_prediction=\"Reject\"\n",
    "        else:\n",
    "            my_prediction=\"Neutral\"\n",
    "        #print(probability[0])  \n",
    "        return( probability , my_prediction   ,my_prediction + \"  --->  Probability of Reject is: \" + \"{0:.0%}\".format(probability[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78e6360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"kl0aK2Yp4orCUZ_d...|So I was in town ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9504\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"kl0aK2Yp4orCUZ_d...|So I was in town ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0f4aJAsBmiGGcySt...|It was crazy busy...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9505\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0f4aJAsBmiGGcySt...|It was crazy busy...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Should output three fields: Probability , the outcome and the full message \n",
    "##list with three elements \n",
    "\n",
    "def myprobability(textbody) : \n",
    "        phrase=list([textbody.decode('utf-8')])\n",
    "        x = tokenizer.texts_to_sequences(phrase)\n",
    "        paddedx = pad_sequences(x, maxlen=120, padding='post', truncating='post')\n",
    "        probability=model.predict_proba(paddedx).max(axis=1) \n",
    "       # print(phrase)\n",
    "        if probability==0:\n",
    "            my_prediction= \"No Classification\"\n",
    "        elif probability<=.3:\n",
    "            my_prediction=\"Accepted\"\n",
    "        elif probability>=.7:\n",
    "            my_prediction=\"Reject\"\n",
    "        else:\n",
    "            my_prediction=\"Neutral\"\n",
    "        #print(probability[0])  \n",
    "        return(probability)\n",
    "    \n",
    "def myprediction(textbody) : \n",
    "        phrase=list([textbody.decode('utf-8')])\n",
    "        x = tokenizer.texts_to_sequences(phrase)\n",
    "        paddedx = pad_sequences(x, maxlen=120, padding='post', truncating='post')\n",
    "        probability=model.predict_proba(paddedx).max(axis=1) \n",
    "       # print(phrase)\n",
    "        if probability==0:\n",
    "            my_prediction= \"No Classification\"\n",
    "        elif probability<=.3:\n",
    "            my_prediction=\"Accepted\"\n",
    "        elif probability>=.7:\n",
    "            my_prediction=\"Reject\"\n",
    "        else:\n",
    "            my_prediction=\"Neutral\"\n",
    "        #print(probability[0])  \n",
    "        return(my_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7621a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 72, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 540, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle 'weakref' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle 'weakref' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'weakref' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1308/1748008608.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyprediction_udf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyprobability_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyprediction_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# and should have a minimal performance impact.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_judf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_create_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mjdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDataType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m     36\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2815\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle 'weakref' object"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9506\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"CuSn1JFQTEt2cK_C...|I went to New Cit...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"CuSn1JFQTEt2cK_C...|I went to New Cit...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9507\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"b7gZ5MZLOP0HZLUj...|Breweries seem to...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"b7gZ5MZLOP0HZLUj...|Breweries seem to...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9508\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"s82DoR3FSIWbcmrK...|I have so many wa...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"s82DoR3FSIWbcmrK...|I have so many wa...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9509\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"xRv-DGAW2qVHWBCK...|I was stranded at...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"xRv-DGAW2qVHWBCK...|I was stranded at...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"ntJUC1wwrXbTeCZP...|I've got to say i...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9510\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"ntJUC1wwrXbTeCZP...|I've got to say i...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"ftzMlpp6R0sngLOJ...|Great experience ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9511\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"ftzMlpp6R0sngLOJ...|Great experience ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9512\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"YP7UT1pILhR_QeuW...|Have been twice a...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"YP7UT1pILhR_QeuW...|Have been twice a...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9513\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"nJ-MlCGD0XkYWdkQ...|A family member w...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"nJ-MlCGD0XkYWdkQ...|A family member w...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9514\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"YRWyWgDEHwk_Q7cp...|Ok place to eat. ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"YRWyWgDEHwk_Q7cp...|Ok place to eat. ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9515\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"-YYpYZMGTzj2tOrI...|It's one of those...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"-YYpYZMGTzj2tOrI...|It's one of those...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0DEp3FSfy3vRwunM...|I've only been he...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9516\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0DEp3FSfy3vRwunM...|I've only been he...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 25\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"P4VF4xQ5BJ4WpEHU...|Good spot to add ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9517\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"P4VF4xQ5BJ4WpEHU...|Good spot to add ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"_SpjTgW_h48I5CBI...|We were staying a...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9518\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"_SpjTgW_h48I5CBI...|We were staying a...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 27\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"u4uT4HPhob3BkBAI...|Love this nail sa...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9519\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"u4uT4HPhob3BkBAI...|Love this nail sa...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 28\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0f4aJAsBmiGGcySt...|It was crazy busy...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9520\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0f4aJAsBmiGGcySt...|It was crazy busy...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 29\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"OkYXvTOrjxILvAhj...|So after cruising...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "myprobability_udf = udf(myprobability, StringType())\n",
    "myprediction_udf = udf(myprediction, StringType())\n",
    "\n",
    "df2 = df2.withColumn(\"probability\", myprobability_udf(\"text\"))\n",
    "df2 = df2.withColumn(\"prediction\", myprediction_udf(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa95964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data flow \n",
    "df2.writeStream.outputMode(\"append\") \\\n",
    "            .format(\"console\") \\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832c2c6",
   "metadata": {},
   "source": [
    "## Stream Through ML Model ( Reading from kafka from python to consume) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e253b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##the data is continuous ??\n",
    "# 1 avoid intergrating spark with Ml model . \n",
    "# 2 how to stream using python api ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c28532",
   "metadata": {},
   "source": [
    "#predicting the streaming kafka messages\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer('ML_Topic',bootstrap_servers=['185.185.126.143:9092'])\n",
    "\n",
    "print(\"Starting ML predictions.\")\n",
    "\n",
    "\n",
    "for message in consumer:\n",
    "    ModelOutcome = modelevdh(message.value.decode('utf-8')) \n",
    "    print(message.value.decode('utf-8').split(\",\")[0])\n",
    "    print(message.value.decode('utf-8').split(\",\")[1])\n",
    "    print(ModelOutcome[0])\n",
    "    print(ModelOutcome[1])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#predicting the streaming kafka messages\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer('ML_Topic',bootstrap_servers=['185.185.126.143:9092'])\n",
    "\n",
    "print(\"Starting ML predictions.\")\n",
    "\n",
    "\n",
    "for message in consumer:\n",
    "    ModelOutcome = modelevdh(message.value.decode('utf-8')) \n",
    "    a=message.value.decode('utf-8').split(\",\")[0]\n",
    "    b=message.value.decode('utf-8').split(\",\")[1]\n",
    "    c=ModelOutcome[0]\n",
    "    d=ModelOutcome[1]\n",
    "    messagesdict = {\"message_key\": a,\"message_text\": b,\"model_score\": c,\"model_outcome\": d}\n",
    "\n",
    "dff = pd.DataFrame.from_dict(messagesdict)\n",
    "print(dff)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef773bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n",
      "-------------------------------------------\n",
      "Batch: 155\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"1GqVyJsXMKAIbywa...|I've eaten here a...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 156\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"kBudygoaz2CbKLc_...|Love this place! ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 157\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"p5gLL6h3oD0RO7J0...|Delicious!! The s...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 158\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"P7FSFtU8rEMllZgi...|I ordered  delive...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 159\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"acYX_YGMjFStuSeu...|For almost half m...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 160\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"ODR8S4vtaCDFcnJz...|One of the worse ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 161\n",
      "-------------------------------------------\n",
      "+--------------------+----------+\n",
      "|           review_id|      text|\n",
      "+--------------------+----------+\n",
      "|\"AoSKz_Aa_ripiqRl...|Great food|\n",
      "+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 162\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"mtms04PQ1alxop-6...|Bartaco is a clas...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 163\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"U0lcXjyEhegq9E2S...|The people that w...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 164\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"QmWEwPgNGMIKCMIL...|Wow!  I decided t...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 165\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"56X_PUKiK8F8MdDJ...|This \\\"dream work...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 166\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"0N79_4wmisKOKgPP...|My go to stop by ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 167\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"45KSuJ2KJkTWeBdu...|Amazing!! From th...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 168\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"MaTKAdNvLwQj4BSo...|Nada is a great s...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 169\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"tD8FeyyGPTfVGvs6...|This is a great o...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 170\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"Wl_EFYu42zJJ_rwx...|This McDonald's i...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 171\n",
      "-------------------------------------------\n",
      "+--------------------+---------+\n",
      "|           review_id|     text|\n",
      "+--------------------+---------+\n",
      "|\"TbzRNdGzJ7MKa6Xb...|Fun place|\n",
      "+--------------------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 172\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"7RPUKCLsZLuT1D83...|We have been relu...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 173\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"IYBiw7JdA9VsHae8...|The food was ok. ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 174\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+\n",
      "|           review_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|\"1yd3aNTqxmoYJt6R...|There isn't ratin...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7149321",
   "metadata": {},
   "source": [
    "# Alert assesment of scores>80% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b96f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d24f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e73be533",
   "metadata": {},
   "source": [
    "# Save all results data set TO Parque file every x minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3f416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
